“If I were not a physicist, I would probably be a musician. I often think in
music. I live my daydreams in music. I see my life in terms of music.” ― Albert
Einstein

Hey New Merchants. Welcome to the first video in a series on the essentials of
music. I'm creating this series with scientists and engineers in mind,
but foremost the musician in you... whether you're just picking up a musical
instrument or have already put in ten thousand hours; there should be something
in store for you.

Now, my objective with this series is to empower you to rediscover and create
music by offering a conceptual and practical understanding of its fundamentals;
however, since music encompasses an infinite variety of sounds with different
cultures creating vastly different musical genres, there's a lot of things that
can mean. 

Music has a lot of ideas that are usually presented to developing musicians as
things to simply practice and commit to muscle memory. Ideas like rhythm and time
signatures; intervals, scales, keys and tuning systems; chord progressions and musical circles,
among other ideas... while incredibly useful, are often discussed without enough
context to allow these musicians to take artistic liberties. 

I am a firm believer in mind and hand, or in learning by doing. This series is
largely meant to supplement your practice, the expectation being that a few
hours with this knowledge and your instrument will be more meaningful than many
hours without it.

Without further ado, let's introduce the defining characteristic of music:
sound. But what is sound?

############### Human physiology/perception of sound and digital/analog/neurological conversions: ###############

Sound is a vibration that travels as a wave through matter like solids, liquids,
and gasses, but typically the medium is air. Notice how the particles in yellow
oscillate back and forth in space to form high and low pressure spots. These
regions of compression and rarefactions aid the propagation of the wave but the
particles themselves are not traveling with the wave.

If we increase the wave's amplitude, the particles cover more distance in the
same amount of time and must move more quickly, but the wave still travels the
same speed.

If we hold the wave speed constant and increase the wave's frequency, the
wavelength shortens as the particles move even faster in order to finish their
round trips more frequently.

But what produces these vibrations?

Well if you're currently listening in on this video, chances are your dynamic loudspeaker.

This variant of loudspeaker has a voice coil attached to a diaphragm and a
permanent magnet at its base and to figure out how this produces sound, we have 
19th century scientist, James Maxwell, who published a set of four elegant
equations, Maxwell's equations, that describe much of classical
electromagnetism. The details of the equations I'll leave to your high school
and college physics courses, or perhaps a future series, but the last two are of
utmost importance to us and describe a beautiful dance. The Maxwell-Faraday
equation on top describes how time-varying magnetic fields generate electric
fields, and implicitly electrical currents, whiles Ampere's Law with Maxwell's
addition, the bottom equation, describes how electrical currents and
time-varying electric fields produce magnetic fields. 

In practice this means a conductive ring moving around a magnet generates a
current as indicated by the small moving spheres in the ring. This produces a
magnetic field that opposes the magnets slowing the ring down. Once the ring
passes by the magnet, the current changes direction in attempts to pull toward
the magnet.

In the case of a solenoid where there is no magnet, a current by itself produces
a magnetic field.

It is from these principles that the electrical signals sent through the voice
coil moves it, and thus the diaphragm, up and down against the permanent magnet.

This conversion of one form of energy or signal into another is called transduction and
electroacoustic tranducers like your loudspeaker carry out these conversions
between electrical signal, magnetic fields, motion of the diaphragm, and
finally air pressure waves... or in the case of my microphone, the same steps
as the speaker but in reverse.

Now dynamic loudspeakers are incredibly inefficient and lose much of their
energy in the form heat. In fact, they convert roughly .5 to 2 percent of the
power they receive into acoustical power! But you know what isn't so
inefficient? Your hearing!

What we interpret as sound is the vibration of the stereocilia in our inner
ears... here's a brief anatomy lesson:

As the waves reach the outer ear, the pinna funnels the sound into the ear
canal to the tympanic membrane, or ear drum. Pressure differences as low as 20
micro Pa, the threshold of hearing, move the ear drum enough to be audible, but
when the pressure exceeds the threshold of pain at 20 Pa hearing loss can occur.

On the other side of the ear drum is the middle ear. The eustachian tube
equalizes the pressure with the outer ear so the ear drum can efficiently move.
When you swallow, or plug your nose and blow, you're opening your eustachian
tube.
Attached to the ear drum is the malleus which connects in series to the incus,
and finally the smallest bone in the body, the stapes. These bones transfer
sound mechanically to the inner ear. If the incoming sound is too loud, a muscle
will react and dampen the sound but since the muscle can fatigue and is
sometimes too slow to react it's best to us hearing protection in loud places.

The inner ear is the last stop before the brain processes sound signal sent to
the cochlear nerve. The snail-looking cochlea is a fluid-filled tube which
connects to the cochlear nerve. The stapes from the middle ear connects to the
oval window at one end of the tube which coils up the snail to a narrow
passageway called the helicotrema before doubling back to the round window at
the other end of the tube. As the stapes vibrates the oval window, the round
window moves in a complementary motion where higher frequencies are heard closer
to the oval window and low frequencies are heard closer to the helicotrema.

Along the length of the cochlea, down the center of the tube, are roughly 30,000
hair cells with stereocilia which sit in a fluid filled with cations. They sit
on top of the thin basilar membrane and span a small gap to connect to the
thick tectorial membrane. When the fluid of the cochlea vibrates from the
stapes, the basilar membrane moves and the hair cells are sheared against the
tectorial membrane allowing cations to enter the hair cells and trigger a
neurological response which is sent upstream to the brain.

From what is first a thought processed by my brain's electrical impulses and
then mechanically sounded by the muscles in my vocal cords and mouth, to the
electrical audio signal received by my microphone digitized by its
analog-digital converter for this video and back to your ears; I personally find
these many stages of conversion that go in to sharing knowledge through video highly 
inspiring. We are, albeit with a few steps, bridging our minds digitally.


################ The physics and properties of sound as it relates to hearing audio: ###############

Now that we've established some of the core ideas to our perception of sound,
let's investigate their geometrical properties.

A sound wave travel through space and time in all directions,
with an amplitude, frequency, and wavelength. The frequency and wavelength of a
sound, together determine the speed of the sound; however, knowing any two of
these properties allow us to determine the third. 

Sound propogates through all states of matter in two specific types of wave forms:
Longitudinal waves and transverse waves (put up animations next to eachother)
In longitudinal, or compression waves, sound may be transferred through all
states of matter and is best described as waves formed by regions of compression
and rarefaction.
(solo diagram pointing out the regions of compression and rarefaction in 2D -- and 3D case if possible).
Transverse waves on the other hand are only possible in solid mediums and are
described as waves of alternating shear forces.
(solo diagram pointout out the shear forces in 2D -- and 3D if possible)

Sound waves, as with electromagnetic waves like light, can be reflected,
refracted, or attenuated by the medium. Just as a varying wavelengths of light
can reflect off a mirror, change angle when going in and out of different
mediums, or be absorbed in solar panels; sound can bounce off of hard surfaces
producing echoes, change direction as it propogates through air and then water,
or be dampened as the sounds energy is absorbed. The attenuation in many media,
such as water and air, is negligible; but is still an interesting phenomenon
nonetheless depending on the viscocity of the medium.

Also relevant to sound and electromagnetic waves are that they travel with varying speed. In
electromagnetic waves, the speed limit is a universal limit, which follows as a
consequence of Einstein's theory of relativity. Nothing with matter can travel
faster than the speed of light in a vaccuum... but as the medium that light
travels through changes, so does the speed of light. The speed of sound is no
different in that the properties of the medium it travels through determine its
speed since the propogation of sound ultimately boils down to atoms bouncing
into each other and produce this wave effect; but it's quite opposite from
electromagnetic waves.
The biggest difference is that light is fastest in a vacuum whereas sound is
slowest, or doesn't propogate at all, in a vacuum. The takeaway here, is that
sound requires matter to propagate its waves.

Now there exists a couple of complex relationships governing the speed of sound
in a medium; namely its pressure, density, heat capacity ratio, temperature,
and motion. For the sake of simplicity, we won't deal with general relativity
and sound, but I will leave resources in the description for the more curious
among you. The Newton-Laplace equation (show equation) deals with first four
relationships. It states that the speed of sound in a medium is proportional to
the square root of the heat capacity ratio and pressure whiles inversely
proptional to the square root of its density. Here, the heat capacity is a
measure of how much heat must be supplied to a given mass of a material to
raise its temperature by a unit; and the ratio involves the heat capactiy at
constant pressure divided by the heat capacity at constant volume. With the
ideal gas law, the assumption is that this ratio doesn't change; however, in a
real gas, both heat capacities increase and continue to differ from each other
by a fixed constant. This fixed difference results in a ratio that decreases
with increasing temperatures.

In the case of a moving medium, the speed of sound changes depending on the
direction of speed, or velocity, relative to a listener. In the case of a tweeting bird
flying in the wind, it's apparent velocity relative to a stationary
listener is the speed of sound as calculated with the Newton-Laplace equation,
in addition to the velocity of the bird and velocity of the wind. (Give animation of a few sample cases)

The speed of a sound is a very important trait when dealing with nuances like
tuning all of the instruments in a band after traveling to a different
elevation above sea-level, listening to a siren coming down a street, or the
classic comedic example of inhaling gasses of varying densities from balloons.

When someone inhales inert gasses like helium, or sulphur-hexafluoride, the
density term in the Newton-Laplace equation changes. If the density is less
than air, as is the case with helium, the speed of sound increases; conversely,
SF6 is more dense than air and slows the speed of sound. The affect on the
listener are interesting changes to the quality of the sound produced. Let's
breakdown the case of inhaling helium and discover why it changes the quality
of sound. Since we've established that the sound waves in the vocal tract
filled with helium is faster than a vocal tract filled with air, and we've also
established that speed of a plane wave is proportional to its frequency and
wavelength, we can conclude that either the frequency, or wavelength, of the
wave changed in order for the speed of sound to have increased. The fundamental
wavelength of the sound depends only on the shape of the vocal tract, and thus
we can reason that the frequency of the sound wave increased. Note, it is
important to distinguish that what's changed here is not the rate at which the
vocal cords are vibrating, but rather the resonant frequency of the vocal
tract. Take a minute to try and reason about gasses with lower densities like
sulfur-hexafluoride producing lower resonant frequencies.

Beyond the physical properties of sound lay the characteristics, or
qualities, that allow listeners to distinguish a variety of sounds. These
include the pitch, timbre, duration, dynamics, texture, and spatial location of
a sound. When dealing with:
########### Characteristics/qualities of sound: ########################################
Pitch:

Pitch, or the fundamental frequency, this stems directly from the vibrating source
of a sound wave. For instance, a string oscillating at a frequency of
110 Hz (times per second) has a fundamental pitch of 110 Hz, just as a vocal
tract, oboe, or bongo, vibrating at 110 Hz also has a fundamental pitch of
110Hz. The key factor that allows a trained listener to differentiate between
the instrument is called its timbre.

Timbre/ADSR:
In the previous example with the balloons, the primary characteristic of sound
that changed was the resonant frequencies of the vocal tract producing the
sound and not the fundamental pitch of the vocal tract itself. This quality is
known in a musical context as timbre, color, or tone, and deals with the varying
degree to which natural harmonics of the instrument are heard. Natural
harmonics are whole number multiples of the fundamental frequency, and the
geometry of the instrument, as well as ambient conditions like the speed of
sound in the vessel creating the sound, define which harmonics are heard the
most. Additionally, the attack, decay, sustain and release of a sound source
help to identify it timbre. For instance, a piano hammer striking a string
produces a distinct attack pattern that is characteristic of a percussive
sound, but short decay and long sustain of amplitude in the wave format allows
listeners to easily differentiate a piano from drum.
(Demonstrate different instruments playing the same fundamental)

Duration:
Duration is the perception of how short, or long, a sound is. Most of the time
this is the time that the sound is first produced to the moment it stops; but
this is not always the case as you might guess that the gaps in my voice are
audible, yet disconnected and do not add up to the full length of audio.

Dynamics:
Dynamics, is simply how quiet, or loud, sound produced by the instrument is
perceived, and is related to the total number of auditory nerve stimulations
from the stereocilia. In short time periods (under 200 ms), a very short sound
can sound softer than a longer sound at when played at the same amplitude,
though this effect is not apparent at longer time intervals. Also in line with
this human illusion, is that the complexity of the audio signal triggers more
nerver stimulation resulting the perception of louder sounds. Compare this
simple sine wave to a saw-tooth wave at the same wave amplitude.


Texture/Spatial location:
When dealing with sonic textures, we begin to move on from single instrument
sound sources to many instruments sound sources and focus on the interactions
and differences between the instruments. I use the term instrument loosely
speaking here, as texture can also refer to sounds generated in the environment
that under conventional thought are not considered instruments such as wind,
static, and rain. Within sonic textures, spatial location comes into play once
we begin to think about where the sound source is coming from. The
aformentioned characteristics of sound, such as timbre, can help to place the
sound of an alto saxophone in the orchestra and determine with certainty the
distance from the source as well as its placement vertically and horizontally
from the listener.

Conclusion:
This video is only an introduction to sound and its properties which allow
music to exist, and what follows are the details for scales, harmony, rhythm,
and more. Remember, I encourage you to apply what you've learned as if you had
first discovered it... and perhaps the next time you hear the humming of a
refrigerator, the screech of a tire burning rubber, or whatever soundscape you
come across in life, you'll think about what is producing the timbre and pitch of
the sound you're hearing, among its other characteristics, and perceive the
musical qualities in what would normally just be considered noise.

In the next video, we'll jump into detail about the frequency range of human
hearing and discuss how this frequency range is used by different cultures to
produce a variety of scales.

If you enjoyed this video, consider liking, subscribing, and sharing the
video with others. Until next time, thanks for watching.


In much the same way that our stereocilia vibrate due to pressure waves, a
condenser microphone's (also called a capacitor microphone) diaphram also
vibrates due to pressure waves (note there many types of microphones that
function a bit differently). The diaphragm in a microphone acts as one of the
two plates of a capacitor and stores a relatively constant electric charge; when
vibrations move the diaphragm, the distance between the diaphragm and other
plate in the capacitor changes which ultimately results in changes to
capacitance and voltage in the circuit.

From the laws surrounding electromagnetism, this displacement of charge in an
electric field acts as an instantaneous change in voltage with closer distances
equating to negative voltage change, and further distances being more positive.
It is this voltage which changes with time that we are measuring when recording
audio signals. When we talk about other electrical instruments, the voltage is
ultimately still being measured with time, but sometimes in a
different manner. For instance, an electric bass doesn't use a diaphragm and
capacitance to measure voltage; but instead relies on magnetism and
inductance to generate alternating current. This current relates to voltage
through Ohm's law, where a majority of the resitance comes from the variable
resistance in the potentiometer, or volume knob. As the volume knob is turned
up, so too does the voltage in the circuit; and as the amplitude of the string
vibration increase with stronger strums, the current increases as with the
voltage. This allows us to consistently use voltages to represent audio signal
which is helpful when storing the signal's information for playback.

The voltage information present in audio signals can be sent to other analog
devices, or converted to a digital representation of the signal through an
audio interface. In contrast to analog signals, digital signals do not retain
as much of the information within an audio signal as they sample voltages from
the continous version of the signal at a set rate. The faster the sampling rate
of the signal, the more accurate the digital copy can represent its analog
counterpart, but it isn't all sampling rate. The bit depth is the quality of
information stored at each sample; with low bit depth, the digital signal
produces a noisy copy of the analog signal and with higher bit depths the noise
added to signal decreases. The true signal may be best represented when it is
visualized directly from the source, but as soon as we attempt to store this
information with analog physical copies like: vinyl discs, or even digital
copies like: WAV or MP3 files, we lose definition in the audio source. One
might imagine that like biological cells performing mitosis over a lifetime and
undergoing mutations that eventually lead to nonfunctional cells, or
photocopying an original document again and again until it's no longer
readable; so too does constant resampling and conversion of an audio signal
result in indiscernible noise.

On the subject of sampling, isn't a wonder why we've established a minimum
sampling rate of 44.1 kHz? It is worth briefly mentioning that human hearing is
only capable of hearing frequencies between roughly 20Hz and 20000Hz. If we
undersampled the audio signal, we would experience aliasing ----------- Something about nyquist rate